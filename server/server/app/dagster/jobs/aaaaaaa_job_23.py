# Generated by workflow API on 2025-05-12T20:38:36.754847
from dagster import job, op, resource, In, Out, Field, StringSource, OpExecutionContext
import pandas as pd
import io
import json
import boto3
from botocore.config import Config
from supabase import Client
from sqlalchemy import create_engine, text
import logging
import os
import datetime
from dotenv import load_dotenv

load_dotenv()

logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

@resource(config_schema={
    "SUPABASE_URL": Field(str, default_value=os.getenv("SUPABASE_URL")),
    "SUPABASE_KEY": Field(str, default_value=os.getenv("SUPABASE_KEY")),
    "S3_ACCESS_KEY_ID": Field(str, default_value=os.getenv("S3_ACCESS_KEY_ID")),
    "S3_SECRET_ACCESS_KEY": Field(str, default_value=os.getenv("S3_SECRET_ACCESS_KEY")),
    "S3_REGION": Field(str, default_value=os.getenv("S3_REGION", "eu-west-2")),
    "S3_ENDPOINT": Field(str, default_value=os.getenv("S3_ENDPOINT", "https://cxdfynepqojqrvfdbuaf.supabase.co/storage/v1/s3"))
})
def supabase_resource(init_context):
    try:
        config = init_context.resource_config
        client = Client(config["SUPABASE_URL"], config["SUPABASE_KEY"])
        client._config = config
        return client
    except Exception as e:
        init_context.log.error(f"Failed to initialize Supabase client: {str(e)}")
        raise

@resource(config_schema={
    "DATABASE_URL": Field(str, default_value=os.getenv("DATABASE_URL"))
})
def db_engine_resource(init_context):
    try:
        return create_engine(init_context.resource_config["DATABASE_URL"])
    except Exception as e:
        init_context.log.error(f"Failed to create database engine: {str(e)}")
        raise

@op(
    required_resource_keys={ "supabase", "db_engine" },
    config_schema={
        "input_file_path": Field(str, description="Path to input file in format 'bucket/path'"),
        "output_file_path": Field(str, description="Path to save output in format 'bucket/path'")
    },
    out=Out(pd.DataFrame),
    name="load_input_23"
)
def load_input_op(context: OpExecutionContext):
    config = context.op_config
    input_file_path = config["input_file_path"]
    context.log.info(f"Loading input file: {input_file_path}")
    
    try:
        supabase_client = context.resources.supabase
        
        s3_access_key = supabase_client._config.get("S3_ACCESS_KEY_ID")
        s3_secret_key = supabase_client._config.get("S3_SECRET_ACCESS_KEY")
        s3_region = supabase_client._config.get("S3_REGION", "eu-west-2")
        s3_endpoint = supabase_client._config.get("S3_ENDPOINT", "https://cxdfynepqojqrvfdbuaf.supabase.co/storage/v1/s3")
        
        if not all([s3_access_key, s3_secret_key]):
            raise ValueError("S3 credentials not provided in config")
        
        if "/" in input_file_path:
            bucket, path = input_file_path.split("/", 1)
        else:
            bucket = "workflow-files"
            path = input_file_path
        
        try:
            s3_client = boto3.client(
                "s3",
                region_name=s3_region,
                endpoint_url=s3_endpoint,
                aws_access_key_id=s3_access_key,
                aws_secret_access_key=s3_secret_key,
                config=Config(s3={"addressing_style": "path"}))
            
            response = s3_client.get_object(Bucket=bucket, Key=path)
            file_content = response["Body"].read()
            
            df = pd.read_csv(io.BytesIO(file_content))
            context.log.info(f"Loaded {len(df)} rows")
            return df
        except Exception as s3_error:
            context.log.error(f"S3 operation failed: {str(s3_error)}")
            raise
    except Exception as e:
        context.log.error(f"Failed to load input: {str(e)}")
        try:
            with context.resources.db_engine.connect() as conn:
                conn.execute(
                    text('INSERT INTO workflow.run_log (dagster_run_id, step_id, log_level, message, timestamp) VALUES (:run_id, :step_id, \'error\', :message, NOW())'),
                    {
                        "run_id": context.run_id,
                        "step_id": "load_input_23",
                        "message": f"Error loading input: {str(e)}"
                    }
                )
                conn.commit()
        except Exception as db_error:
            context.log.error(f"Failed to log error: {str(db_error)}")
        raise

@op(
    ins={ "input_df": In(pd.DataFrame) },
    out=Out(dict),
    config_schema={
        "parameters": Field(dict, is_required=False, default_value={}),
        "step_label": Field(str, is_required=False, default_value="transform_step")
    },
    name="transform_23"
)
def transform_op(context: OpExecutionContext, input_df: pd.DataFrame):
    try:
        context.log.info(f"Transforming {len(input_df)} rows")
        
        result = {
            "data": input_df.to_dict(orient="records"),
            "metadata": {
                "workflow_id": 23,
                "transformed_at": datetime.datetime.now().isoformat()
            }
        }
        
        context.log.info("Transformation completed successfully")
        return result
    except Exception as e:
        context.log.error(f"Transformation failed: {str(e)}")
        raise

@op(
    required_resource_keys={ "supabase" },
    ins={ "transformed_data": In(dict) },
    config_schema={
        "output_file_path": Field(str, description="Path to save output in format 'bucket/path'")
    },
    name="save_output_23"
)
def save_output_op(context: OpExecutionContext, transformed_data: dict):
    output_file_path = context.op_config["output_file_path"]
    context.log.info(f"Saving output to {output_file_path}")
    
    try:
        supabase_client = context.resources.supabase
        
        s3_access_key = supabase_client._config.get("S3_ACCESS_KEY_ID")
        s3_secret_key = supabase_client._config.get("S3_SECRET_ACCESS_KEY")
        s3_region = supabase_client._config.get("S3_REGION", "eu-west-2")
        s3_endpoint = supabase_client._config.get("S3_ENDPOINT", "https://cxdfynepqojqrvfdbuaf.supabase.co/storage/v1/s3")
        
        if not all([s3_access_key, s3_secret_key]):
            raise ValueError("S3 credentials not provided in config")
        
        if "/" in output_file_path:
            bucket, path = output_file_path.split("/", 1)
        else:
            bucket = "workflow-files"
            path = output_file_path
        
        try:
            s3_client = boto3.client(
                "s3",
                region_name=s3_region,
                endpoint_url=s3_endpoint,
                aws_access_key_id=s3_access_key,
                aws_secret_access_key=s3_secret_key,
                config=Config(s3={"addressing_style": "path"}))
            
            json_data = json.dumps(transformed_data, indent=2).encode("utf-8")
            
            s3_client.put_object(
                Bucket=bucket,
                Key=path,
                Body=json_data,
                ContentType="application/json"
            )
            
            context.log.info(f"Successfully saved output")
        except Exception as s3_error:
            context.log.error(f"S3 operation failed: {str(s3_error)}")
            raise
    except Exception as e:
        context.log.error(f"Failed to save output: {str(e)}")
        raise

@job(
    name="aaaaaaa_job_23",
    resource_defs={
        "supabase": supabase_resource,
        "db_engine": db_engine_resource
    },
    tags={
        "workflow_id": "23",
        "workflow_name": "aaaaaaa",
        "generated_at": "2025-05-12T20:38:36.754847"
    }
)
def aaaaaaa_job_23():
    input_df = load_input_23()
    transformed_data = transform_23(input_df)
    save_output_23(transformed_data)
