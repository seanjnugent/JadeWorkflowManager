# Generated by workflow API on 2025-05-21T18:17:23.581429
from dagster import job, op, resource, In, Out, Field, StringSource, OpExecutionContext
import pandas as pd
import io
import datetime
import json
import boto3
from botocore.config import Config
from supabase import Client
from sqlalchemy import create_engine
import os
from dotenv import load_dotenv

load_dotenv()

@resource(config_schema={
    "SUPABASE_URL": Field(str, default_value=os.getenv("SUPABASE_URL")),
    "SUPABASE_KEY": Field(str, default_value=os.getenv("SUPABASE_KEY")),
    "S3_ACCESS_KEY_ID": Field(str, default_value=os.getenv("S3_ACCESS_KEY_ID")),
    "S3_SECRET_ACCESS_KEY": Field(str, default_value=os.getenv("S3_SECRET_ACCESS_KEY")),
    "S3_REGION": Field(str, default_value=os.getenv("S3_REGION", "eu-west-2")),
    "S3_ENDPOINT": Field(str, default_value=os.getenv("S3_ENDPOINT", "https://cxdfynepqojqrvfdbuaf.supabase.co/storage/v1/s3"))
})
def supabase_resource(init_context):
    try:
        config = init_context.resource_config
        client = Client(config["SUPABASE_URL"], config["SUPABASE_KEY"])
        client._config = config
        return client
    except Exception as e:
        init_context.log.error(f"Failed to initialize Supabase client: {str(e)}")
        raise

@resource(config_schema={
    "DATABASE_URL": Field(str, default_value=os.getenv("DATABASE_URL"))
})
def db_engine_resource(init_context):
    try:
        return create_engine(init_context.resource_config["DATABASE_URL"])
    except Exception as e:
        init_context.log.error(f"Failed to create database engine: {str(e)}")
        raise

@op(
    required_resource_keys={ "supabase", "db_engine" },
    config_schema={
        "input_file_path": Field(str, description="Path to input file in format 'bucket/path'"),
        "output_file_path": Field(str, description="Path to save output in format 'bucket/path'"),
        "workflow_id": Field(int, description="ID of the workflow"),
    },
    out=Out(pd.DataFrame),
    name="load_input_csv_to_json_converter_new_53"
)
def load_input_op(context: OpExecutionContext):
    config = context.op_config
    input_file_path = config["input_file_path"]
    workflow_id = config["workflow_id"]
    context.log.info(f"Loading input file: {input_file_path} for workflow {workflow_id}")
    
    try:
        supabase_client = context.resources.supabase
        
        s3_access_key = supabase_client._config.get("S3_ACCESS_KEY_ID")
        s3_secret_key = supabase_client._config.get("S3_SECRET_ACCESS_KEY")
        s3_region = supabase_client._config.get("S3_REGION", "eu-west-2")
        s3_endpoint = supabase_client._config.get("S3_ENDPOINT", "https://cxdfynepqojqrvfdbuaf.supabase.co/storage/v1/s3")
        
        if not all([s3_access_key, s3_secret_key]):
            raise ValueError("S3 credentials not provided in config")
        
        if "/" in input_file_path:
            bucket, path = input_file_path.split("/", 1)
        else:
            bucket = "workflow-files"
            path = input_file_path
        
        s3_client = boto3.client(
            "s3",
            region_name=s3_region,
            endpoint_url=s3_endpoint,
            aws_access_key_id=s3_access_key,
            aws_secret_access_key=s3_secret_key,
            config=Config(s3={"addressing_style": "path"}))
        
        response = s3_client.get_object(Bucket=bucket, Key=path)
        file_content = response["Body"].read()
        
        df = pd.read_csv(io.BytesIO(file_content))
        context.log.info(f"Loaded {len(df)} rows")
        return df
    except Exception as e:
        context.log.error(f"Failed to load input: {str(e)}")
        raise

@op(
    ins={ "input_df": In(pd.DataFrame) },
    out=Out(dict),
    config_schema={
        "parameters": Field(dict, is_required=False, default_value={}),
        "workflow_id": Field(int, description="ID of the workflow"),
        "step_label": Field(str, is_required=False, default_value="transform_step")
    },
    name="transform_csv_to_json_converter_new_53"
)
def transform_op(context: OpExecutionContext, input_df: pd.DataFrame):
    workflow_id = context.op_config["workflow_id"]
    context.log.info(f"Transforming {len(input_df)} rows for workflow {workflow_id}")
    
    try:
        result = {
            "data": input_df.to_dict(orient="records"),
            "metadata": {
                "workflow_id": workflow_id,
                "transformed_at": datetime.datetime.now().isoformat()
            }
        }
        context.log.info("Transformation completed successfully")
        return result
    except Exception as e:
        context.log.error(f"Transformation failed: {str(e)}")
        raise

@op(
    required_resource_keys={ "supabase", "db_engine" },
    ins={ "transformed_data": In(dict) },
    config_schema={
        "output_file_path": Field(str, description="Path to save output in format 'bucket/path'"),
        "workflow_id": Field(int, description="ID of the workflow")
    },
    name="save_output_csv_to_json_converter_new_53"
)
def save_output_op(context: OpExecutionContext, transformed_data: dict):
    output_file_path = context.op_config["output_file_path"]
    workflow_id = context.op_config["workflow_id"]
    context.log.info(f"Saving output to {output_file_path} for workflow {workflow_id}")
    
    try:
        supabase_client = context.resources.supabase
        
        s3_access_key = supabase_client._config.get("S3_ACCESS_KEY_ID")
        s3_secret_key = supabase_client._config.get("S3_SECRET_ACCESS_KEY")
        s3_region = supabase_client._config.get("S3_REGION", "eu-west-2")
        s3_endpoint = supabase_client._config.get("S3_ENDPOINT", "https://cxdfynepqojqrvfdbuaf.supabase.co/storage/v1/s3")
        
        if not all([s3_access_key, s3_secret_key]):
            raise ValueError("S3 credentials not provided in config")
        
        if "/" in output_file_path:
            bucket, path = output_file_path.split("/", 1)
        else:
            bucket = "workflow-files"
            path = output_file_path
        
        s3_client = boto3.client(
            "s3",
            region_name=s3_region,
            endpoint_url=s3_endpoint,
            aws_access_key_id=s3_access_key,
            aws_secret_access_key=s3_secret_key,
            config=Config(s3={"addressing_style": "path"}))
        
        json_data = json.dumps(transformed_data, indent=2).encode("utf-8")
        
        s3_client.put_object(
            Bucket=bucket,
            Key=path,
            Body=json_data,
            ContentType="application/json"
        )
        
        context.log.info("Successfully saved output")
    except Exception as e:
        context.log.error(f"Failed to save output: {str(e)}")
        raise

@job(
    name="csv_to_json_converter_new_job_53",
    resource_defs={
        "supabase": supabase_resource,
        "db_engine": db_engine_resource
    },
    config={
        "ops": {
            "load_input_csv_to_json_converter_new_53": {
                "config": {
                    "input_file_path": "workflow-files/inputs/{{ run_id }}.csv",
                    "output_file_path": "workflow-files/outputs/{{ run_id }}.json",
                    "workflow_id": 53
                }
            },
            "transform_csv_to_json_converter_new_53": {
                "config": {
                    "workflow_id": 53,
                    "parameters": {} # These could be populated from workflow config
                }
            },
            "save_output_csv_to_json_converter_new_53": {
                "config": {
                    "output_file_path": "workflow-files/outputs/{{ run_id }}.json",
                    "workflow_id": 53
                }
            }
        }
    },
    tags={
        "workflow_id": "53",
        "workflow_name": "csv_to_json_converter_new",
        "generated_at": "2025-05-21T18:17:23.581429",
        "dag_filename": "csv_to_json_converter_new_job_53.py"
    }
)
def workflow_job():
    input_df = load_input_op()
    transformed_data = transform_op(input_df)
    save_output_op(transformed_data)
