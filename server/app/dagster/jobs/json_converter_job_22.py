# Generated by workflow API on 2025-05-14T19:50:19.869361
from dagster import job, op, resource, In, Out, Field, StringSource, OpExecutionContext
import pandas as pd
import io
import json
import boto3
from botocore.config import Config
from supabase import Client
from sqlalchemy import create_engine, text
import logging
import os
import datetime
from dotenv import load_dotenv

load_dotenv()

logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

@resource(config_schema={
    "SUPABASE_URL": Field(str, default_value=os.getenv("SUPABASE_URL")),
    "SUPABASE_KEY": Field(str, default_value=os.getenv("SUPABASE_KEY")),
    "S3_ACCESS_KEY_ID": Field(str, default_value=os.getenv("S3_ACCESS_KEY_ID")),
    "S3_SECRET_ACCESS_KEY": Field(str, default_value=os.getenv("S3_SECRET_ACCESS_KEY")),
    "S3_REGION": Field(str, default_value=os.getenv("S3_REGION", "eu-west-2")),
    "S3_ENDPOINT": Field(str, default_value=os.getenv("S3_ENDPOINT", "https://cxdfynepqojqrvfdbuaf.supabase.co/storage/v1/s3"))
})
def supabase_resource(init_context):
    try:
        config = init_context.resource_config
        client = Client(config["SUPABASE_URL"], config["SUPABASE_KEY"])
        client._config = config
        return client
    except Exception as e:
        init_context.log.error(f"Failed to initialize Supabase client: {str(e)}")
        raise

@resource(config_schema={
    "DATABASE_URL": Field(str, default_value=os.getenv("DATABASE_URL"))
})
def db_engine_resource(init_context):
    try:
        return create_engine(init_context.resource_config["DATABASE_URL"])
    except Exception as e:
        init_context.log.error(f"Failed to create database engine: {str(e)}")
        raise

@op(
    required_resource_keys={ "supabase", "db_engine" },
    config_schema={
        "input_file_path": Field(str, description="Path to input file in format 'bucket/path'"),
        "output_file_path": Field(str, description="Path to save output in format 'bucket/path'"),
        "workflow_id": Field(int, description="ID of the workflow"),
    },
    out=Out(pd.DataFrame),
    name="load_input_json_converter_22"
)
def load_input_op(context: OpExecutionContext):
    config = context.op_config
    input_file_path = config["input_file_path"]
    workflow_id = config["workflow_id"]
    context.log.info(f"Loading input file: {input_file_path} for workflow {workflow_id}")
    
    try:
        supabase_client = context.resources.supabase
        
        s3_access_key = supabase_client._config.get("S3_ACCESS_KEY_ID")
        s3_secret_key = supabase_client._config.get("S3_SECRET_ACCESS_KEY")
        s3_region = supabase_client._config.get("S3_REGION", "eu-west-2")
        s3_endpoint = supabase_client._config.get("S3_ENDPOINT", "https://cxdfynepqojqrvfdbuaf.supabase.co/storage/v1/s3")
        
        if not all([s3_access_key, s3_secret_key]):
            raise ValueError("S3 credentials not provided in config")
        
        if "/" in input_file_path:
            bucket, path = input_file_path.split("/", 1)
        else:
            bucket = "workflow-files"
            path = input_file_path
        
        try:
            s3_client = boto3.client(
                "s3",
                region_name=s3_region,
                endpoint_url=s3_endpoint,
                aws_access_key_id=s3_access_key,
                aws_secret_access_key=s3_secret_key,
                config=Config(s3={"addressing_style": "path"}))
            
            response = s3_client.get_object(Bucket=bucket, Key=path)
            file_content = response["Body"].read()
            
            df = pd.read_csv(io.BytesIO(file_content))
            context.log.info(f"Loaded {len(df)} rows")
            
            # Log success to database with workflow_id
            try:
                with context.resources.db_engine.connect() as conn:
                    conn.execute(
                        text('INSERT INTO workflow.run_log (dagster_run_id, workflow_id, step_id, log_level, message, timestamp) VALUES (:run_id, :workflow_id, :step_id, \'info\', :message, NOW())'),
                        {
                            "run_id": context.run_id,
                            "workflow_id": workflow_id, 
                            "step_id": "load_input_json_converter_22",
                            "message": f"Successfully loaded {len(df)} rows"
                        }
                    )
                    conn.commit()
            except Exception as db_error:
                context.log.error(f"Failed to log success: {str(db_error)}")
            
            return df
        except Exception as s3_error:
            context.log.error(f"S3 operation failed: {str(s3_error)}")
            raise
    except Exception as e:
        context.log.error(f"Failed to load input: {str(e)}")
        try:
            with context.resources.db_engine.connect() as conn:
                conn.execute(
                    text('INSERT INTO workflow.run_log (dagster_run_id, workflow_id, step_id, log_level, message, timestamp) VALUES (:run_id, :workflow_id, :step_id, \'error\', :message, NOW())'),
                    {
                        "run_id": context.run_id,
                        "workflow_id": workflow_id,
                        "step_id": "load_input_json_converter_22",
                        "message": f"Error loading input: {str(e)}"
                    }
                )
                conn.commit()
        except Exception as db_error:
            context.log.error(f"Failed to log error: {str(db_error)}")
        raise

@op(
    ins={ "input_df": In(pd.DataFrame) },
    out=Out(dict),
    config_schema={
        "parameters": Field(dict, is_required=False, default_value={}),
        "workflow_id": Field(int, description="ID of the workflow"),
        "step_label": Field(str, is_required=False, default_value="transform_step")
    },
    name="transform_json_converter_22"
)
def transform_op(context: OpExecutionContext, input_df: pd.DataFrame):
    workflow_id = context.op_config["workflow_id"]
    try:
        context.log.info(f"Transforming {len(input_df)} rows for workflow {workflow_id}")
        
        result = {
            "data": input_df.to_dict(orient="records"),
            "metadata": {
                "workflow_id": workflow_id,
                "transformed_at": datetime.datetime.now().isoformat()
            }
        }
        
        # Log success to database
        try:
            with context.resources.db_engine.connect() as conn:
                conn.execute(
                    text('INSERT INTO workflow.run_log (dagster_run_id, workflow_id, step_id, log_level, message, timestamp) VALUES (:run_id, :workflow_id, :step_id, \'info\', :message, NOW())'),
                    {
                        "run_id": context.run_id,
                        "workflow_id": workflow_id,
                        "step_id": "transform_json_converter_22",
                        "message": "Transformation completed successfully"
                    }
                )
                conn.commit()
        except Exception as db_error:
            context.log.error(f"Failed to log success: {str(db_error)}")
        
        context.log.info("Transformation completed successfully")
        return result
    except Exception as e:
        context.log.error(f"Transformation failed: {str(e)}")
        # Log error to database
        try:
            with context.resources.db_engine.connect() as conn:
                conn.execute(
                    text('INSERT INTO workflow.run_log (dagster_run_id, workflow_id, step_id, log_level, message, timestamp) VALUES (:run_id, :workflow_id, :step_id, \'error\', :message, NOW())'),
                    {
                        "run_id": context.run_id,
                        "workflow_id": workflow_id,
                        "step_id": "transform_json_converter_22",
                        "message": f"Transformation failed: {str(e)}"
                    }
                )
                conn.commit()
        except Exception as db_error:
            context.log.error(f"Failed to log error: {str(db_error)}")
        raise

@op(
    required_resource_keys={ "supabase", "db_engine" },
    ins={ "transformed_data": In(dict) },
    config_schema={
        "output_file_path": Field(str, description="Path to save output in format 'bucket/path'"),
        "workflow_id": Field(int, description="ID of the workflow")
    },
    name="save_output_json_converter_22"
)
def save_output_op(context: OpExecutionContext, transformed_data: dict):
    output_file_path = context.op_config["output_file_path"]
    workflow_id = context.op_config["workflow_id"]
    context.log.info(f"Saving output to {output_file_path} for workflow {workflow_id}")
    
    try:
        supabase_client = context.resources.supabase
        
        s3_access_key = supabase_client._config.get("S3_ACCESS_KEY_ID")
        s3_secret_key = supabase_client._config.get("S3_SECRET_ACCESS_KEY")
        s3_region = supabase_client._config.get("S3_REGION", "eu-west-2")
        s3_endpoint = supabase_client._config.get("S3_ENDPOINT", "https://cxdfynepqojqrvfdbuaf.supabase.co/storage/v1/s3")
        
        if not all([s3_access_key, s3_secret_key]):
            raise ValueError("S3 credentials not provided in config")
        
        if "/" in output_file_path:
            bucket, path = output_file_path.split("/", 1)
        else:
            bucket = "workflow-files"
            path = output_file_path
        
        try:
            s3_client = boto3.client(
                "s3",
                region_name=s3_region,
                endpoint_url=s3_endpoint,
                aws_access_key_id=s3_access_key,
                aws_secret_access_key=s3_secret_key,
                config=Config(s3={"addressing_style": "path"}))
            
            json_data = json.dumps(transformed_data, indent=2).encode("utf-8")
            
            s3_client.put_object(
                Bucket=bucket,
                Key=path,
                Body=json_data,
                ContentType="application/json"
            )
            
            # Update workflow status in database
            try:
                with context.resources.db_engine.connect() as conn:
                    # Update workflow status
                    conn.execute(
                        text('UPDATE workflow.workflow SET status = ''completed'', last_run_at = NOW() WHERE id = :workflow_id'),
                        {"workflow_id": workflow_id}
                    )
                    
                    # Log success
                    conn.execute(
                        text('INSERT INTO workflow.run_log (dagster_run_id, workflow_id, step_id, log_level, message, timestamp) VALUES (:run_id, :workflow_id, :step_id, \'info\', :message, NOW())'),
                        {
                            "run_id": context.run_id,
                            "workflow_id": workflow_id,
                            "step_id": "save_output_json_converter_22",
                            "message": "Successfully saved output"
                        }
                    )
                    conn.commit()
            except Exception as db_error:
                context.log.error(f"Failed to update workflow status: {str(db_error)}")
            
            context.log.info(f"Successfully saved output")
        except Exception as s3_error:
            context.log.error(f"S3 operation failed: {str(s3_error)}")
            raise
    except Exception as e:
        context.log.error(f"Failed to save output: {str(e)}")
        # Log error to database
        try:
            with context.resources.db_engine.connect() as conn:
                conn.execute(
                    text('INSERT INTO workflow.run_log (dagster_run_id, workflow_id, step_id, log_level, message, timestamp) VALUES (:run_id, :workflow_id, :step_id, \'error\', :message, NOW())'),
                    {
                        "run_id": context.run_id,
                        "workflow_id": workflow_id,
                        "step_id": "save_output_json_converter_22",
                        "message": f"Failed to save output: {str(e)}"
                    }
                )
                conn.commit()
        except Exception as db_error:
            context.log.error(f"Failed to log error: {str(db_error)}")
        raise

@job(
    name="json_converter_job_22",
    resource_defs={
        "supabase": supabase_resource,
        "db_engine": db_engine_resource
    },
    config={
        "ops": {
            "load_input_json_converter_22": {
                "config": {
                    "input_file_path": "workflow-files/inputs/{{ run_id }}.csv",
                    "output_file_path": "workflow-files/outputs/{{ run_id }}.json",
                    "workflow_id": 22
                }
            },
            "transform_json_converter_22": {
                "config": {
                    "workflow_id": 22,
                    "parameters": {} # These could be populated from workflow config
                }
            },
            "save_output_json_converter_22": {
                "config": {
                    "output_file_path": "workflow-files/outputs/{{ run_id }}.json",
                    "workflow_id": 22
                }
            }
        }
    },
    tags={
        "workflow_id": "22",
        "workflow_name": "json_converter",
        "generated_at": "2025-05-14T19:50:19.869361",
        "dag_filename": "json_converter_job_22.py"
    }
)
def workflow_job():
    input_df = load_input_op()
    transformed_data = transform_op(input_df)
    save_output_op(transformed_data)
