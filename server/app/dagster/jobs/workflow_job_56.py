# Generated by workflow API on 2025-05-14T19:50:19.869361
from dagster import job, op, resource, In, Out, Field, StringSource, OpExecutionContext
import pandas as pd
import io
import json
import boto3
from botocore.config import Config
from supabase import Client
from sqlalchemy import create_engine, text
import logging
import os
import datetime
from dotenv import load_dotenv

load_dotenv()

logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

@resource(config_schema={
    "SUPABASE_URL": Field(str, default_value=os.getenv("SUPABASE_URL")),
    "SUPABASE_KEY": Field(str, default_value=os.getenv("SUPABASE_KEY")),
    "S3_ACCESS_KEY_ID": Field(str, default_value=os.getenv("S3_ACCESS_KEY_ID")),
    "S3_SECRET_ACCESS_KEY": Field(str, default_value=os.getenv("S3_SECRET_ACCESS_KEY")),
    "S3_REGION": Field(str, default_value=os.getenv("S3_REGION", "eu-west-2")),
    "S3_ENDPOINT": Field(str, default_value=os.getenv("S3_ENDPOINT", "https://cxdfynepqojqrvfdbuaf.supabase.co/storage/v1/s3"))
})
def supabase_resource(init_context):
    try:
        config = init_context.resource_config
        client = Client(config["SUPABASE_URL"], config["SUPABASE_KEY"])
        client._config = config
        return client
    except Exception as e:
        init_context.log.error(f"Failed to initialize Supabase client: {str(e)}")
        raise

@resource(config_schema={
    "DATABASE_URL": Field(str, default_value=os.getenv("DATABASE_URL"))
})
def db_engine_resource(init_context):
    try:
        return create_engine(init_context.resource_config["DATABASE_URL"])
    except Exception as e:
        init_context.log.error(f"Failed to create database engine: {str(e)}")
        raise

@op(
    required_resource_keys={"supabase", "db_engine"},
    config_schema={
        "input_file_path": Field(str, description="Path to input file in format 'bucket/path'"),
        "output_file_path": Field(str, description="Path to save output in format 'bucket/path'"),
        "workflow_id": Field(int, description="ID of the workflow"),
    },
    out=Out(pd.DataFrame),
    name="load_input_json_converter_56"
)
def load_input_op(context: OpExecutionContext):
    """Load input CSV file from S3."""
    config = context.op_config
    input_file_path = config["input_file_path"]
    workflow_id = config["workflow_id"]
    context.log.info(f"Loading input file: {input_file_path} for workflow {workflow_id}")

    try:
        supabase_client = context.resources.supabase
        s3_access_key = supabase_client._config.get("S3_ACCESS_KEY_ID")
        s3_secret_key = supabase_client._config.get("S3_SECRET_ACCESS_KEY")
        s3_region = supabase_client._config.get("S3_REGION", "eu-west-2")
        s3_endpoint = supabase_client._config.get("S3_ENDPOINT")

        if not all([s3_access_key, s3_secret_key, s3_endpoint]):
            raise ValueError("Missing S3 credentials or endpoint in config")

        bucket, path = input_file_path.split("/", 1) if "/" in input_file_path else ("workflow-files", input_file_path)
        s3_client = boto3.client(
            "s3",
            region_name=s3_region,
            endpoint_url=s3_endpoint,
            aws_access_key_id=s3_access_key,
            aws_secret_key=s3_secret_key,
            config=Config(s3={"addressing_style": "path"})
        )

        response = s3_client.get_object(Bucket=bucket, Key=path)
        df = pd.read_csv(io.BytesIO(response["Body"].read()))
        context.log.info(f"Loaded {len(df)} rows")

        with context.resources.db_engine.connect() as conn:
            conn.execute(
                text("""
                INSERT INTO workflow.run_log (dagster_run_id, workflow_id, step_code, log_level, message, timestamp)
                VALUES (:run_id, :workflow_id, :step_id, 'info', :message, NOW())
                """),
                {
                    "run_id": context.run_id,
                    "workflow_id": workflow_id,
                    "step_id": "load_input_json_converter_56",
                    "message": f"Successfully loaded {len(df)} rows"
                }
            )
            conn.commit()

        return df

    except Exception as e:
        context.log.error(f"Failed to load input: {str(e)}")
        try:
            with context.resources.db_engine.connect() as conn:
                conn.execute(
                    text("""
                    INSERT INTO workflow.run_log (dagster_run_id, workflow_id, step_code, log_level, message, timestamp)
                    VALUES (:run_id, :workflow_id, :step_id, 'error', :message, NOW())
                    """),
                    {
                        "run_id": context.run_id,
                        "workflow_id": workflow_id,
                        "step_id": "load_input_json_converter_56",
                        "message": f"Error loading input: {str(e)}"
                    }
                )
                conn.commit()
        except Exception as db_error:
            context.log.error(f"Failed to log error: {str(db_error)}")
        raise

@op(
    ins={"input_df": In(pd.DataFrame)},
    out=Out(dict),
    config_schema={
        "parameters": Field(dict, is_required=False, default_value={}),
        "workflow_id": Field(int, description="ID of the workflow"),
        "step_label": Field(str, is_required=False, default_value="transform_step")
    },
    name="transform_json_converter_56"
)
def transform_op(context: OpExecutionContext, input_df: pd.DataFrame):
    """Transform DataFrame to JSON-compatible dictionary."""
    workflow_id = context.op_config["workflow_id"]
    try:
        context.log.info(f"Transforming {len(input_df)} rows for workflow {workflow_id}")
        result = {
            "data": input_df.to_dict(orient="records"),
            "metadata": {
                "workflow_id": workflow_id,
                "transformed_at": datetime.datetime.now().isoformat()
            }
        }

        with context.resources.db_engine.connect() as conn:
            conn.execute(
                text("""
                INSERT INTO workflow.run_log (dagster_run_id, workflow_id, step_code, log_level, message, timestamp)
                VALUES (:run_id, :workflow_id, :step_id, 'info', :message, NOW())
                """),
                {
                    "run_id": context.run_id,
                    "workflow_id": workflow_id,
                    "step_id": "transform_json_converter_56",
                    "message": "Transformation completed successfully"
                }
            )
            conn.commit()

        return result

    except Exception as e:
        context.log.error(f"Transformation failed: {str(e)}")
        try:
            with context.resources.db_engine.connect() as conn:
                conn.execute(
                    text("""
                    INSERT INTO workflow.run_log (dagster_run_id, workflow_id, step_code, log_level, message, timestamp)
                    VALUES (:run_id, :workflow_id, :step_id, 'error', :message, NOW())
                    """),
                    {
                        "run_id": context.run_id,
                        "workflow_id": workflow_id,
                        "step_id": "transform_json_converter_56",
                        "message": f"Transformation failed: {str(e)}"
                    }
                )
                conn.commit()
        except Exception as db_error:
            context.log.error(f"Failed to log error: {str(db_error)}")
        raise

@op(
    required_resource_keys={"supabase", "db_engine"},
    ins={"transformed_data": In(dict)},
    config_schema={
        "output_file_path": Field(str, description="Path to save output in format 'bucket/path'"),
        "workflow_id": Field(int, description="ID of the workflow")
    },
    name="save_output_json_converter_56"
)
def save_output_op(context: OpExecutionContext, transformed_data: dict):
    """Save transformed data to S3 as JSON."""
    output_file_path = context.op_config["output_file_path"]
    workflow_id = context.op_config["workflow_id"]
    context.log.info(f"Saving output to {output_file_path} for workflow {workflow_id}")

    try:
        supabase_client = context.resources.supabase
        s3_access_key = supabase_client._config.get("S3_ACCESS_KEY_ID")
        s3_secret_key = supabase_client._config.get("S3_SECRET_ACCESS_KEY")
        s3_region = supabase_client._config.get("S3_REGION", "eu-west-2")
        s3_endpoint = supabase_client._config.get("S3_ENDPOINT")

        if not all([s3_access_key, s3_secret_key, s3_endpoint]):
            raise ValueError("Missing S3 credentials or endpoint in config")

        bucket, path = output_file_path.split("/", 1) if "/" in output_file_path else ("workflow-files", output_file_path)
        s3_client = boto3.client(
            "s3",
            region_name=s3_region,
            endpoint_url=s3_endpoint,
            aws_access_key_id=s3_access_key,
            aws_secret_key=s3_secret_key,
            config=Config(s3={"addressing_style": "path"})
        )

        json_data = json.dumps(transformed_data, indent=2).encode("utf-8")
        s3_client.put_object(
            Bucket=bucket,
            Key=path,
            Body=json_data,
            ContentType="application/json"
        )

        with context.resources.db_engine.connect() as conn:
            conn.execute(
                text("""
                UPDATE workflow.workflow
                SET status = 'completed', last_run_at = NOW()
                WHERE id = :workflow_id
                """),
                {"workflow_id": workflow_id}
            )
            conn.execute(
                text("""
                INSERT INTO workflow.run_log (dagster_run_id, workflow_id, step_code, log_level, message, timestamp)
                VALUES (:run_id, :workflow_id, :step_id, 'info', :message, NOW())
                """),
                {
                    "run_id": context.run_id,
                    "workflow_id": workflow_id,
                    "step_id": "save_output_json_converter_56",
                    "message": "Successfully saved output"
                }
            )
            conn.commit()

        context.log.info("Successfully saved output")

    except Exception as e:
        context.log.error(f"Failed to save output: {str(e)}")
        try:
            with context.resources.db_engine.connect() as conn:
                conn.execute(
                    text("""
                    INSERT INTO workflow.run_log (dagster_run_id, workflow_id, step_code, log_level, message, timestamp)
                    VALUES (:run_id, :workflow_id, :step_id, 'error', :message, NOW())
                    """),
                    {
                        "run_id": context.run_id,
                        "workflow_id": workflow_id,
                        "step_id": "save_output_json_converter_56",
                        "message": f"Failed to save output: {str(e)}"
                    }
                )
                conn.commit()
        except Exception as db_error:
            context.log.error(f"Failed to log error: {str(db_error)}")
        raise

@job(
    name="workflow_job_56",
    resource_defs={
        "supabase": supabase_resource,
        "db_engine": db_engine_resource
    },
    config={
        "ops": {
            "load_input_json_converter_56": {
                "config": {
                    "input_file_path": "",
                    "output_file_path": "",
                    "workflow_id": 56
                }
            },
            "transform_json_converter_56": {
                "config": {
                    "workflow_id": 56,
                    "parameters": {}
                }
            },
            "save_output_json_converter_56": {
                "config": {
                    "output_file_path": "",
                    "workflow_id": 56
                }
            }
        }
    },
    tags={
        "workflow_id": "56",
        "workflow_name": "json_converter",
        "generated_at": "2025-06-14T15:38:00.000000",
        "dag_filename": "workflow_job_56.py"
    }
)
def workflow_job():
    """Define the workflow job pipeline."""
    input_df = load_input_op()
    transformed_data = transform_op(input_df)
    save_output_op(transformed_data)