from fastapi import APIRouter, Depends, HTTPException, status
from fastapi.responses import JSONResponse
from sqlalchemy.orm import Session
from sqlalchemy import text
from pydantic import BaseModel
import os
import json
import logging
from typing import Dict, List, Any, Optional
import re
from datetime import datetime
from ..get_health_check import get_db
from dotenv import load_dotenv
from pathlib import Path

load_dotenv()

logger = logging.getLogger(__name__)

router = APIRouter(prefix="/dags", tags=["dags"])


# Construct the absolute path relative to this file's location
DAG_OUTPUT_DIR = Path(__file__).resolve().parent.parent / "app" / "dagster" / "jobs"

os.makedirs(DAG_OUTPUT_DIR, exist_ok=True)

class DagCreate(BaseModel):
    workflow_id: int
    workflow_name: str

# Updated template with fully unique operation names that incorporate workflow_id AND safe_name
DAG_TEMPLATE = """# Generated by workflow API on {timestamp}
from dagster import job, op, resource, In, Out, Field, StringSource, OpExecutionContext
import pandas as pd
import io
import json
import boto3
from botocore.config import Config
from supabase import Client
from sqlalchemy import create_engine, text
import logging
import os
import datetime
from dotenv import load_dotenv

load_dotenv()

logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

@resource(config_schema={{
    "SUPABASE_URL": Field(str, default_value=os.getenv("SUPABASE_URL")),
    "SUPABASE_KEY": Field(str, default_value=os.getenv("SUPABASE_KEY")),
    "S3_ACCESS_KEY_ID": Field(str, default_value=os.getenv("S3_ACCESS_KEY_ID")),
    "S3_SECRET_ACCESS_KEY": Field(str, default_value=os.getenv("S3_SECRET_ACCESS_KEY")),
    "S3_REGION": Field(str, default_value=os.getenv("S3_REGION", "eu-west-2")),
    "S3_ENDPOINT": Field(str, default_value=os.getenv("S3_ENDPOINT", "https://cxdfynepqojqrvfdbuaf.supabase.co/storage/v1/s3"))
}})
def supabase_resource(init_context):
    try:
        config = init_context.resource_config
        client = Client(config["SUPABASE_URL"], config["SUPABASE_KEY"])
        client._config = config
        return client
    except Exception as e:
        init_context.log.error(f"Failed to initialize Supabase client: {{str(e)}}")
        raise

@resource(config_schema={{
    "DATABASE_URL": Field(str, default_value=os.getenv("DATABASE_URL"))
}})
def db_engine_resource(init_context):
    try:
        return create_engine(init_context.resource_config["DATABASE_URL"])
    except Exception as e:
        init_context.log.error(f"Failed to create database engine: {{str(e)}}")
        raise

@op(
    required_resource_keys={{ "supabase", "db_engine" }},
    config_schema={{
        "input_file_path": Field(str, description="Path to input file in format 'bucket/path'"),
        "output_file_path": Field(str, description="Path to save output in format 'bucket/path'"),
        "workflow_id": Field(int, description="ID of the workflow"),
    }},
    out=Out(pd.DataFrame),
    name="load_input_{safe_name}_{workflow_id}"
)
def load_input_op(context: OpExecutionContext):
    config = context.op_config
    input_file_path = config["input_file_path"]
    workflow_id = config["workflow_id"]
    context.log.info(f"Loading input file: {{input_file_path}} for workflow {{workflow_id}}")
    
    try:
        supabase_client = context.resources.supabase
        
        s3_access_key = supabase_client._config.get("S3_ACCESS_KEY_ID")
        s3_secret_key = supabase_client._config.get("S3_SECRET_ACCESS_KEY")
        s3_region = supabase_client._config.get("S3_REGION", "eu-west-2")
        s3_endpoint = supabase_client._config.get("S3_ENDPOINT", "https://cxdfynepqojqrvfdbuaf.supabase.co/storage/v1/s3")
        
        if not all([s3_access_key, s3_secret_key]):
            raise ValueError("S3 credentials not provided in config")
        
        if "/" in input_file_path:
            bucket, path = input_file_path.split("/", 1)
        else:
            bucket = "workflow-files"
            path = input_file_path
        
        try:
            s3_client = boto3.client(
                "s3",
                region_name=s3_region,
                endpoint_url=s3_endpoint,
                aws_access_key_id=s3_access_key,
                aws_secret_access_key=s3_secret_key,
                config=Config(s3={{"addressing_style": "path"}}))
            
            response = s3_client.get_object(Bucket=bucket, Key=path)
            file_content = response["Body"].read()
            
            df = pd.read_csv(io.BytesIO(file_content))
            context.log.info(f"Loaded {{len(df)}} rows")
            
            # Log success to database with workflow_id
            try:
                with context.resources.db_engine.connect() as conn:
                    conn.execute(
                        text('INSERT INTO workflow.run_log (dagster_run_id, workflow_id, step_id, log_level, message, timestamp) VALUES (:run_id, :workflow_id, :step_id, \\'info\\', :message, NOW())'),
                        {{
                            "run_id": context.run_id,
                            "workflow_id": workflow_id, 
                            "step_id": "load_input_{safe_name}_{workflow_id}",
                            "message": f"Successfully loaded {{len(df)}} rows"
                        }}
                    )
                    conn.commit()
            except Exception as db_error:
                context.log.error(f"Failed to log success: {{str(db_error)}}")
            
            return df
        except Exception as s3_error:
            context.log.error(f"S3 operation failed: {{str(s3_error)}}")
            raise
    except Exception as e:
        context.log.error(f"Failed to load input: {{str(e)}}")
        try:
            with context.resources.db_engine.connect() as conn:
                conn.execute(
                    text('INSERT INTO workflow.run_log (dagster_run_id, workflow_id, step_id, log_level, message, timestamp) VALUES (:run_id, :workflow_id, :step_id, \\'error\\', :message, NOW())'),
                    {{
                        "run_id": context.run_id,
                        "workflow_id": workflow_id,
                        "step_id": "load_input_{safe_name}_{workflow_id}",
                        "message": f"Error loading input: {{str(e)}}"
                    }}
                )
                conn.commit()
        except Exception as db_error:
            context.log.error(f"Failed to log error: {{str(db_error)}}")
        raise

@op(
    ins={{ "input_df": In(pd.DataFrame) }},
    out=Out(dict),
    config_schema={{
        "parameters": Field(dict, is_required=False, default_value={{}}),
        "workflow_id": Field(int, description="ID of the workflow"),
        "step_label": Field(str, is_required=False, default_value="transform_step")
    }},
    name="transform_{safe_name}_{workflow_id}"
)
def transform_op(context: OpExecutionContext, input_df: pd.DataFrame):
    workflow_id = context.op_config["workflow_id"]
    try:
        context.log.info(f"Transforming {{len(input_df)}} rows for workflow {{workflow_id}}")
        
        result = {{
            "data": input_df.to_dict(orient="records"),
            "metadata": {{
                "workflow_id": workflow_id,
                "transformed_at": datetime.datetime.now().isoformat()
            }}
        }}
        
        # Log success to database
        try:
            with context.resources.db_engine.connect() as conn:
                conn.execute(
                    text('INSERT INTO workflow.run_log (dagster_run_id, workflow_id, step_id, log_level, message, timestamp) VALUES (:run_id, :workflow_id, :step_id, \\'info\\', :message, NOW())'),
                    {{
                        "run_id": context.run_id,
                        "workflow_id": workflow_id,
                        "step_id": "transform_{safe_name}_{workflow_id}",
                        "message": "Transformation completed successfully"
                    }}
                )
                conn.commit()
        except Exception as db_error:
            context.log.error(f"Failed to log success: {{str(db_error)}}")
        
        context.log.info("Transformation completed successfully")
        return result
    except Exception as e:
        context.log.error(f"Transformation failed: {{str(e)}}")
        # Log error to database
        try:
            with context.resources.db_engine.connect() as conn:
                conn.execute(
                    text('INSERT INTO workflow.run_log (dagster_run_id, workflow_id, step_id, log_level, message, timestamp) VALUES (:run_id, :workflow_id, :step_id, \\'error\\', :message, NOW())'),
                    {{
                        "run_id": context.run_id,
                        "workflow_id": workflow_id,
                        "step_id": "transform_{safe_name}_{workflow_id}",
                        "message": f"Transformation failed: {{str(e)}}"
                    }}
                )
                conn.commit()
        except Exception as db_error:
            context.log.error(f"Failed to log error: {{str(db_error)}}")
        raise

@op(
    required_resource_keys={{ "supabase", "db_engine" }},
    ins={{ "transformed_data": In(dict) }},
    config_schema={{
        "output_file_path": Field(str, description="Path to save output in format 'bucket/path'"),
        "workflow_id": Field(int, description="ID of the workflow")
    }},
    name="save_output_{safe_name}_{workflow_id}"
)
def save_output_op(context: OpExecutionContext, transformed_data: dict):
    output_file_path = context.op_config["output_file_path"]
    workflow_id = context.op_config["workflow_id"]
    context.log.info(f"Saving output to {{output_file_path}} for workflow {{workflow_id}}")
    
    try:
        supabase_client = context.resources.supabase
        
        s3_access_key = supabase_client._config.get("S3_ACCESS_KEY_ID")
        s3_secret_key = supabase_client._config.get("S3_SECRET_ACCESS_KEY")
        s3_region = supabase_client._config.get("S3_REGION", "eu-west-2")
        s3_endpoint = supabase_client._config.get("S3_ENDPOINT", "https://cxdfynepqojqrvfdbuaf.supabase.co/storage/v1/s3")
        
        if not all([s3_access_key, s3_secret_key]):
            raise ValueError("S3 credentials not provided in config")
        
        if "/" in output_file_path:
            bucket, path = output_file_path.split("/", 1)
        else:
            bucket = "workflow-files"
            path = output_file_path
        
        try:
            s3_client = boto3.client(
                "s3",
                region_name=s3_region,
                endpoint_url=s3_endpoint,
                aws_access_key_id=s3_access_key,
                aws_secret_access_key=s3_secret_key,
                config=Config(s3={{"addressing_style": "path"}}))
            
            json_data = json.dumps(transformed_data, indent=2).encode("utf-8")
            
            s3_client.put_object(
                Bucket=bucket,
                Key=path,
                Body=json_data,
                ContentType="application/json"
            )
            
            # Update workflow status in database
            try:
                with context.resources.db_engine.connect() as conn:
                    # Update workflow status
                    conn.execute(
                        text('UPDATE workflow.workflow SET status = \''completed\'', last_run_at = NOW() WHERE id = :workflow_id'),
                        {{"workflow_id": workflow_id}}
                    )
                    
                    # Log success
                    conn.execute(
                        text('INSERT INTO workflow.run_log (dagster_run_id, workflow_id, step_id, log_level, message, timestamp) VALUES (:run_id, :workflow_id, :step_id, \\'info\\', :message, NOW())'),
                        {{
                            "run_id": context.run_id,
                            "workflow_id": workflow_id,
                            "step_id": "save_output_{safe_name}_{workflow_id}",
                            "message": "Successfully saved output"
                        }}
                    )
                    conn.commit()
            except Exception as db_error:
                context.log.error(f"Failed to update workflow status: {{str(db_error)}}")
            
            context.log.info(f"Successfully saved output")
        except Exception as s3_error:
            context.log.error(f"S3 operation failed: {{str(s3_error)}}")
            raise
    except Exception as e:
        context.log.error(f"Failed to save output: {{str(e)}}")
        # Log error to database
        try:
            with context.resources.db_engine.connect() as conn:
                conn.execute(
                    text('INSERT INTO workflow.run_log (dagster_run_id, workflow_id, step_id, log_level, message, timestamp) VALUES (:run_id, :workflow_id, :step_id, \\'error\\', :message, NOW())'),
                    {{
                        "run_id": context.run_id,
                        "workflow_id": workflow_id,
                        "step_id": "save_output_{safe_name}_{workflow_id}",
                        "message": f"Failed to save output: {{str(e)}}"
                    }}
                )
                conn.commit()
        except Exception as db_error:
            context.log.error(f"Failed to log error: {{str(db_error)}}")
        raise

@job(
    name="{workflow_name}_job_{workflow_id}",
    resource_defs={{
        "supabase": supabase_resource,
        "db_engine": db_engine_resource
    }},
    config={{
        "ops": {{
            "load_input_{safe_name}_{workflow_id}": {{
                "config": {{
                    "input_file_path": "workflow-files/inputs/{{{{ run_id }}}}.csv",
                    "output_file_path": "workflow-files/outputs/{{{{ run_id }}}}.json",
                    "workflow_id": {workflow_id}
                }}
            }},
            "transform_{safe_name}_{workflow_id}": {{
                "config": {{
                    "workflow_id": {workflow_id},
                    "parameters": {{}} # These could be populated from workflow config
                }}
            }},
            "save_output_{safe_name}_{workflow_id}": {{
                "config": {{
                    "output_file_path": "workflow-files/outputs/{{{{ run_id }}}}.json",
                    "workflow_id": {workflow_id}
                }}
            }}
        }}
    }},
    tags={{
        "workflow_id": "{workflow_id}",
        "workflow_name": "{workflow_name}",
        "generated_at": "{timestamp}",
        "dag_filename": "{dag_filename}"
    }}
)
def workflow_job():
    input_df = load_input_op()
    transformed_data = transform_op(input_df)
    save_output_op(transformed_data)
"""

def sanitize_name(name: str) -> str:
    # Create a safer, more restricted sanitized name
    return re.sub(r'[^a-zA-Z0-9_]', '_', name.lower())

def generate_dag_filename(workflow_id: int, workflow_name: str) -> str:
    safe_name = sanitize_name(workflow_name)
    return f"{safe_name}_job_{workflow_id}.py"

def create_dag_file(workflow_id: int, workflow_name: str) -> str:
    try:
        safe_name = sanitize_name(workflow_name)
        filename = generate_dag_filename(workflow_id, workflow_name)
        
        content = DAG_TEMPLATE.format(
            workflow_id=workflow_id,
            workflow_name=safe_name,
            safe_name=safe_name,  # Include safe_name for operation naming
            timestamp=datetime.now().isoformat(),
            dag_filename=filename
        )
        
        filepath = os.path.join(DAG_OUTPUT_DIR, filename)
        
        with open(filepath, "w") as f:
            f.write(content)
        
        # Update database to mark DAG as ready
        try:
            db = next(get_db())
            db.execute(
                text('UPDATE workflow.workflow SET dag_status = :status, dag_path = :path, updated_at = NOW() WHERE id = :workflow_id'),
                {
                    "status": "ready",
                    "path": filepath,
                    "workflow_id": workflow_id
                }
            )
            db.commit()
        except Exception as db_error:
            logger.error(f"Failed to update workflow DAG status: {str(db_error)}")
            
        return filepath
    except Exception as e:
        logger.error(f"Failed to create DAG file: {str(e)}")
        raise

@router.post("/dag/new", status_code=status.HTTP_201_CREATED)
async def create_dag_endpoint(
    request: DagCreate,
    db: Session = Depends(get_db)
):
    try:
        required_env_vars = [
            "SUPABASE_URL", "SUPABASE_KEY", 
            "S3_ACCESS_KEY_ID", "S3_SECRET_ACCESS_KEY",
            "DATABASE_URL"
        ]
        missing_vars = [var for var in required_env_vars if not os.getenv(var)]
        if missing_vars:
            raise HTTPException(
                status_code=status.HTTP_400_BAD_REQUEST,
                detail=f"Missing required environment variables: {', '.join(missing_vars)}"
            )
        
        # Check if DAG already exists
        existing_dag_query = text('SELECT dag_path FROM workflow.workflow WHERE id = :workflow_id')
        result = db.execute(existing_dag_query, {"workflow_id": request.workflow_id}).fetchone()
        
        if result and result.dag_path and os.path.exists(result.dag_path):
            # DAG already exists, you might want to update it or return status
            return {
                "status": "exists",
                "workflow_id": request.workflow_id,
                "dag_path": result.dag_path,
                "message": f"DAG already exists for workflow {request.workflow_id}"
            }
        
        dag_path = create_dag_file(
            workflow_id=request.workflow_id,
            workflow_name=request.workflow_name
        )
        
        return {
            "status": "success",
            "workflow_id": request.workflow_id,
            "dag_path": dag_path,
            "message": f"DAG created successfully for workflow {request.workflow_id}"
        }
    except HTTPException:
        raise
    except Exception as e:
        logger.error(f"Error creating DAG: {str(e)}")
        # Update workflow status to indicate failure
        try:
            db.execute(
                text('UPDATE workflow.workflow SET dag_status = :status, error_message = :error, updated_at = NOW() WHERE id = :workflow_id'),
                {
                    "status": "error", 
                    "error": str(e)[:500],  # Truncate long error messages
                    "workflow_id": request.workflow_id
                }
            )
            db.commit()
        except Exception as db_error:
            logger.error(f"Failed to update workflow error status: {str(db_error)}")
            
        raise HTTPException(
            status_code=status.HTTP_500_INTERNAL_SERVER_ERROR,
            detail=f"Failed to create DAG: {str(e)}"
        )

# Additional endpoints for workflow management

@router.get("/dag/{workflow_id}", status_code=status.HTTP_200_OK)
async def get_dag_status(
    workflow_id: int,
    db: Session = Depends(get_db)
):
    try:
        query = text('SELECT id, name as workflow_name, dag_status, dag_path FROM workflow.workflow WHERE id = :workflow_id')
        result = db.execute(query, {"workflow_id": workflow_id}).fetchone()
        
        if not result:
            raise HTTPException(
                status_code=status.HTTP_404_NOT_FOUND,
                detail=f"Workflow with ID {workflow_id} not found"
            )
        
        return {
            "workflow_id": result.id,
            "workflow_name": result.workflow_name,
            "dag_status": result.dag_status,
            "dag_path": result.dag_path
        }
    except HTTPException:
        raise
    except Exception as e:
        logger.error(f"Error getting DAG status: {str(e)}")
        raise HTTPException(
            status_code=status.HTTP_500_INTERNAL_SERVER_ERROR,
            detail=f"Failed to get DAG status: {str(e)}"
        )