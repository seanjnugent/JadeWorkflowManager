from fastapi import APIRouter, Depends, HTTPException, status
from pydantic import BaseModel
import os
import logging
from typing import Dict, List, Any, Optional
import re
from datetime import datetime
from sqlalchemy.orm import Session
from sqlalchemy import text
from ..get_health_check import get_db
from dotenv import load_dotenv
from pathlib import Path

load_dotenv()

logger = logging.getLogger(__name__)

router = APIRouter(prefix="/dags", tags=["dags"])

# Construct the absolute path relative to this file's location
DAG_OUTPUT_DIR = Path(__file__).resolve().parent.parent.parent / "app" / "dagster" / "jobs"
os.makedirs(DAG_OUTPUT_DIR, exist_ok=True)

class DagCreate(BaseModel):
    workflow_id: int
    workflow_name: str

def validate_environment():
    required_vars = [
        "S3_ACCESS_KEY_ID", "S3_SECRET_ACCESS_KEY", "S3_REGION", "S3_BUCKET",
        "DATABASE_URL"
    ]
    missing_vars = [var for var in required_vars if not os.getenv(var)]
    if missing_vars:
        raise RuntimeError(f"Missing required environment variables: {', '.join(missing_vars)}")

# Validate environment at startup
validate_environment()

def sanitize_name(name: str) -> str:
    """Sanitize workflow name for use in filenames and identifiers"""
    return re.sub(r'[^a-zA-Z0-9_]', '_', name.lower())

def generate_dag_filename(workflow_id: int) -> str:
    """Generate consistent DAG filename using workflow ID"""
    return f"workflow_job_{workflow_id}.py"

def create_dag_file(workflow_id: int, workflow_name: str) -> str:
    """Create a new DAG file for the workflow"""
    try:
        safe_name = sanitize_name(workflow_name)
        filename = generate_dag_filename(workflow_id)
        filepath = os.path.join(DAG_OUTPUT_DIR, filename)
        
        # Use consistent job name pattern
        job_name = f"workflow_job_{workflow_id}"
        
        # Use the DAG template content with consistent naming
        dag_template = """# Generated by workflow API on {timestamp}
from dagster import job, op, resource, In, Out, Field, StringSource, OpExecutionContext
import pandas as pd
import io
import datetime
import json
import boto3
from botocore.config import Config
from sqlalchemy import create_engine
import os
from dotenv import load_dotenv

load_dotenv()

@resource(config_schema={{
    "S3_ACCESS_KEY_ID": Field(str, default_value=os.getenv("S3_ACCESS_KEY_ID")),
    "S3_SECRET_ACCESS_KEY": Field(str, default_value=os.getenv("S3_SECRET_ACCESS_KEY")),
    "S3_REGION": Field(str, default_value=os.getenv("S3_REGION", "eu-west-2")),
    "S3_BUCKET": Field(str, default_value=os.getenv("S3_BUCKET"))
}})
def s3_resource(init_context):
    try:
        config = init_context.resource_config
        s3_client = boto3.client(
            "s3",
            region_name=config["S3_REGION"],
            aws_access_key_id=config["S3_ACCESS_KEY_ID"],
            aws_secret_access_key=config["S3_SECRET_ACCESS_KEY"],
            config=Config(s3={{"addressing_style": "path"}})
        )
        return s3_client
    except Exception as e:
        init_context.log.error(f"Failed to initialize S3 client: {{str(e)}}")
        raise

@resource(config_schema={{
    "DATABASE_URL": Field(str, default_value=os.getenv("DATABASE_URL"))
}})
def db_engine_resource(init_context):
    try:
        return create_engine(init_context.resource_config["DATABASE_URL"])
    except Exception as e:
        init_context.log.error(f"Failed to create database engine: {{str(e)}}")
        raise

@op(
    required_resource_keys={{ "s3", "db_engine" }},
    config_schema={{
        "input_file_path": Field(str, description="Path to input file in format 'path'"),
        "output_file_path": Field(str, description="Path to save output in format 'path'"),
        "workflow_id": Field(int, description="ID of the workflow"),
    }},
    out=Out(pd.DataFrame),
    name="load_input_{job_name}"
)
def load_input_op(context: OpExecutionContext):
    config = context.op_config
    input_file_path = config["input_file_path"]
    workflow_id = config["workflow_id"]
    context.log.info(f"Loading input file: {{input_file_path}} for workflow {{workflow_id}}")
    
    try:
        s3_client = context.resources.s3
        s3_bucket = s3_client._config.get("S3_BUCKET")
        
        if not s3_bucket:
            raise ValueError("S3 bucket not provided in config")
        
        response = s3_client.get_object(Bucket=s3_bucket, Key=input_file_path)
        file_content = response["Body"].read()
        
        df = pd.read_csv(io.BytesIO(file_content))
        context.log.info(f"Loaded {{len(df)}} rows")
        return df
    except Exception as e:
        context.log.error(f"Failed to load input: {{str(e)}}")
        raise

@op(
    ins={{ "input_df": In(pd.DataFrame) }},
    out=Out(dict),
    config_schema={{
        "parameters": Field(dict, is_required=False, default_value={{}}),
        "workflow_id": Field(int, description="ID of the workflow"),
        "step_label": Field(str, is_required=False, default_value="transform_step")
    }},
    name="transform_{job_name}"
)
def transform_op(context: OpExecutionContext, input_df: pd.DataFrame):
    workflow_id = context.op_config["workflow_id"]
    context.log.info(f"Transforming {{len(input_df)}} rows for workflow {{workflow_id}}")
    
    try:
        result = {{
            "data": input_df.to_dict(orient="records"),
            "metadata": {{
                "workflow_id": workflow_id,
                "transformed_at": datetime.datetime.now().isoformat()
            }}
        }}
        context.log.info("Transformation completed successfully")
        return result
    except Exception as e:
        context.log.error(f"Transformation failed: {{str(e)}}")
        raise

@op(
    required_resource_keys={{ "s3", "db_engine" }},
    ins={{ "transformed_data": In(dict) }},
    config_schema={{
        "output_file_path": Field(str, description="Path to save output in format 'path'"),
        "workflow_id": Field(int, description="ID of the workflow")
    }},
    name="save_output_{job_name}"
)
def save_output_op(context: OpExecutionContext, transformed_data: dict):
    output_file_path = context.op_config["output_file_path"]
    workflow_id = context.op_config["workflow_id"]
    context.log.info(f"Saving output to {{output_file_path}} for workflow {{workflow_id}}")
    
    try:
        s3_client = context.resources.s3
        s3_bucket = s3_client._config.get("S3_BUCKET")
        
        if not s3_bucket:
            raise ValueError("S3 bucket not provided in config")
        
        json_data = json.dumps(transformed_data, indent=2).encode("utf-8")
        
        s3_client.put_object(
            Bucket=s3_bucket,
            Key=output_file_path,
            Body=json_data,
            ContentType="application/json"
        )
        
        context.log.info("Successfully saved output")
    except Exception as e:
        context.log.error(f"Failed to save output: {{str(e)}}")
        raise

@job(
    name="{job_name}",
    resource_defs={{
        "s3": s3_resource,
        "db_engine": db_engine_resource
    }},
    config={{
        "ops": {{
            "load_input_{job_name}": {{
                "config": {{
                    "input_file_path": "inputs/{{{{ run_id }}}}.csv",
                    "output_file_path": "outputs/{{{{ run_id }}}}.json",
                    "workflow_id": {workflow_id}
                }}
            }},
            "transform_{job_name}": {{
                "config": {{
                    "workflow_id": {workflow_id},
                    "parameters": {{}} # These could be populated from workflow config
                }}
            }},
            "save_output_{job_name}": {{
                "config": {{
                    "output_file_path": "outputs/{{{{ run_id }}}}.json",
                    "workflow_id": {workflow_id}
                }}
            }}
        }}
    }},
    tags={{
        "workflow_id": "{workflow_id}",
        "workflow_name": "{workflow_name}",
        "generated_at": "{timestamp}",
        "dag_filename": "{dag_filename}"
    }}
)
def workflow_job():
    input_df = load_input_op()
    transformed_data = transform_op(input_df)
    save_output_op(transformed_data)
"""
        
        content = dag_template.format(
            workflow_id=workflow_id,
            workflow_name=safe_name,
            job_name=job_name,
            timestamp=datetime.now().isoformat(),
            dag_filename=filename
        )
        
        with open(filepath, "w") as f:
            f.write(content)
        
        # Update workflow record in database
        try:
            db = next(get_db())
            db.execute(
                text('''
                    UPDATE workflow.workflow 
                    SET dag_status = :status, 
                        dag_path = :path, 
                        updated_at = NOW(), 
                        status = :workflow_status 
                    WHERE id = :workflow_id
                '''),
                {
                    "status": "ready",
                    "workflow_status": "Active",
                    "path": filepath,
                    "workflow_id": workflow_id
                }
            )
            db.commit()
        except Exception as db_error:
            logger.error(f"Failed to update workflow DAG status: {str(db_error)}")
            raise
        
        return filepath
    except Exception as e:
        logger.error(f"Failed to create DAG file: {str(e)}")
        raise

@router.post("/dag/new", status_code=status.HTTP_201_CREATED)
async def create_dag_endpoint(
    request: DagCreate,
    db: Session = Depends(get_db)
):
    """Endpoint to create a new DAG file for a workflow"""
    try:
        validate_environment()
        
        # Check if DAG already exists
        existing_dag_query = text('''
            SELECT dag_path FROM workflow.workflow 
            WHERE id = :workflow_id
        ''')
        result = db.execute(existing_dag_query, {"workflow_id": request.workflow_id}).fetchone()
        
        if result and result.dag_path and os.path.exists(result.dag_path):
            return {
                "status": "exists",
                "workflow_id": request.workflow_id,
                "dag_path": result.dag_path,
                "message": f"DAG already exists for workflow {request.workflow_id}"
            }
        
        # Create new DAG file
        dag_path = create_dag_file(
            workflow_id=request.workflow_id,
            workflow_name=request.workflow_name
        )
        
        return {
            "status": "success",
            "workflow_id": request.workflow_id,
            "dag_path": dag_path,
            "message": f"DAG created successfully for workflow {request.workflow_id}"
        }
    except HTTPException:
        raise
    except Exception as e:
        logger.error(f"Error creating DAG: {str(e)}")
        try:
            db.execute(
                text('''
                    UPDATE workflow.workflow 
                    SET dag_status = :status, 
                        error_message = :error, 
                        updated_at = NOW() 
                    WHERE id = :workflow_id
                '''),
                {
                    "status": "error", 
                    "error": str(e)[:500],
                    "workflow_id": request.workflow_id
                }
            )
            db.commit()
        except Exception as db_error:
            logger.error(f"Failed to update workflow error status: {str(db_error)}")
            
        raise HTTPException(
            status_code=status.HTTP_500_INTERNAL_SERVER_ERROR,
            detail=f"Failed to create DAG: {str(e)}"
        )